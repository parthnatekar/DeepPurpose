{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "import DeepPurpose.DTI as models\n",
    "from DeepPurpose.utils import *\n",
    "from DeepPurpose.dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset from path...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 1249049: expected 193 fields, saw 205\\nSkipping line 1249075: expected 193 fields, saw 205\\n'\n",
      "b'Skipping line 1281769: expected 193 fields, saw 205\\n'\n",
      "b'Skipping line 1304439: expected 193 fields, saw 265\\n'\n",
      "b'Skipping line 1708248: expected 193 fields, saw 241\\n'\n",
      "b'Skipping line 1822154: expected 193 fields, saw 241\\nSkipping line 1822155: expected 193 fields, saw 241\\nSkipping line 1822156: expected 193 fields, saw 241\\nSkipping line 1822157: expected 193 fields, saw 241\\nSkipping line 1822158: expected 193 fields, saw 241\\nSkipping line 1822159: expected 193 fields, saw 241\\nSkipping line 1822160: expected 193 fields, saw 241\\nSkipping line 1822161: expected 193 fields, saw 241\\nSkipping line 1822162: expected 193 fields, saw 241\\n'\n",
      "b'Skipping line 1835427: expected 193 fields, saw 205\\n'\n",
      "b'Skipping line 1854650: expected 193 fields, saw 241\\nSkipping line 1854651: expected 193 fields, saw 241\\nSkipping line 1854652: expected 193 fields, saw 241\\nSkipping line 1854653: expected 193 fields, saw 241\\nSkipping line 1854654: expected 193 fields, saw 241\\nSkipping line 1854655: expected 193 fields, saw 241\\nSkipping line 1854656: expected 193 fields, saw 241\\nSkipping line 1854657: expected 193 fields, saw 241\\nSkipping line 1854658: expected 193 fields, saw 241\\n'\n",
      "b'Skipping line 1893828: expected 193 fields, saw 205\\n'\n",
      "b'Skipping line 2280301: expected 193 fields, saw 265\\n'\n",
      "b'Skipping line 2367522: expected 193 fields, saw 265\\nSkipping line 2367523: expected 193 fields, saw 265\\nSkipping line 2367524: expected 193 fields, saw 265\\nSkipping line 2367525: expected 193 fields, saw 265\\nSkipping line 2367526: expected 193 fields, saw 265\\nSkipping line 2367527: expected 193 fields, saw 265\\nSkipping line 2367528: expected 193 fields, saw 265\\nSkipping line 2367529: expected 193 fields, saw 265\\nSkipping line 2367530: expected 193 fields, saw 265\\nSkipping line 2367531: expected 193 fields, saw 265\\nSkipping line 2367532: expected 193 fields, saw 265\\nSkipping line 2367533: expected 193 fields, saw 265\\nSkipping line 2367534: expected 193 fields, saw 265\\nSkipping line 2367535: expected 193 fields, saw 265\\nSkipping line 2367536: expected 193 fields, saw 265\\nSkipping line 2367537: expected 193 fields, saw 265\\nSkipping line 2367538: expected 193 fields, saw 265\\nSkipping line 2367539: expected 193 fields, saw 265\\nSkipping line 2367540: expected 193 fields, saw 265\\nSkipping line 2368158: expected 193 fields, saw 265\\nSkipping line 2368159: expected 193 fields, saw 265\\nSkipping line 2368160: expected 193 fields, saw 265\\nSkipping line 2368161: expected 193 fields, saw 265\\nSkipping line 2368162: expected 193 fields, saw 265\\nSkipping line 2368163: expected 193 fields, saw 265\\nSkipping line 2368164: expected 193 fields, saw 265\\n'\n",
      "b'Skipping line 2379575: expected 193 fields, saw 265\\nSkipping line 2379576: expected 193 fields, saw 265\\nSkipping line 2379577: expected 193 fields, saw 265\\nSkipping line 2379578: expected 193 fields, saw 265\\nSkipping line 2379579: expected 193 fields, saw 265\\nSkipping line 2379580: expected 193 fields, saw 265\\nSkipping line 2379581: expected 193 fields, saw 265\\nSkipping line 2379582: expected 193 fields, saw 265\\nSkipping line 2379583: expected 193 fields, saw 265\\nSkipping line 2379584: expected 193 fields, saw 265\\nSkipping line 2379585: expected 193 fields, saw 265\\nSkipping line 2379586: expected 193 fields, saw 265\\nSkipping line 2379587: expected 193 fields, saw 265\\nSkipping line 2379588: expected 193 fields, saw 265\\nSkipping line 2379589: expected 193 fields, saw 265\\nSkipping line 2379590: expected 193 fields, saw 265\\nSkipping line 2379591: expected 193 fields, saw 265\\nSkipping line 2379592: expected 193 fields, saw 265\\nSkipping line 2379593: expected 193 fields, saw 265\\nSkipping line 2379594: expected 193 fields, saw 265\\nSkipping line 2379595: expected 193 fields, saw 265\\nSkipping line 2379596: expected 193 fields, saw 265\\nSkipping line 2379597: expected 193 fields, saw 265\\nSkipping line 2379598: expected 193 fields, saw 265\\nSkipping line 2379599: expected 193 fields, saw 265\\nSkipping line 2379600: expected 193 fields, saw 265\\nSkipping line 2379601: expected 193 fields, saw 265\\nSkipping line 2379602: expected 193 fields, saw 265\\nSkipping line 2379603: expected 193 fields, saw 265\\nSkipping line 2379604: expected 193 fields, saw 265\\nSkipping line 2379605: expected 193 fields, saw 265\\nSkipping line 2379606: expected 193 fields, saw 265\\nSkipping line 2379607: expected 193 fields, saw 265\\nSkipping line 2379608: expected 193 fields, saw 265\\nSkipping line 2379609: expected 193 fields, saw 265\\nSkipping line 2379610: expected 193 fields, saw 265\\nSkipping line 2379611: expected 193 fields, saw 265\\nSkipping line 2379612: expected 193 fields, saw 265\\nSkipping line 2379613: expected 193 fields, saw 265\\n'\n",
      "b'Skipping line 2382210: expected 193 fields, saw 265\\nSkipping line 2382211: expected 193 fields, saw 265\\nSkipping line 2382212: expected 193 fields, saw 265\\nSkipping line 2382213: expected 193 fields, saw 265\\nSkipping line 2382214: expected 193 fields, saw 265\\nSkipping line 2382215: expected 193 fields, saw 265\\nSkipping line 2382216: expected 193 fields, saw 265\\nSkipping line 2382217: expected 193 fields, saw 265\\n'\n",
      "b'Skipping line 2394765: expected 193 fields, saw 313\\n'\n",
      "b'Skipping line 2412309: expected 193 fields, saw 637\\n'\n",
      "b'Skipping line 2462659: expected 193 fields, saw 385\\n'\n",
      "b'Skipping line 2478523: expected 193 fields, saw 265\\nSkipping line 2478524: expected 193 fields, saw 265\\nSkipping line 2478525: expected 193 fields, saw 265\\nSkipping line 2478526: expected 193 fields, saw 265\\nSkipping line 2478527: expected 193 fields, saw 265\\n'\n",
      "b'Skipping line 2490277: expected 193 fields, saw 265\\nSkipping line 2490278: expected 193 fields, saw 265\\nSkipping line 2490279: expected 193 fields, saw 265\\n'\n",
      "b'Skipping line 2530419: expected 193 fields, saw 265\\nSkipping line 2530420: expected 193 fields, saw 265\\nSkipping line 2530421: expected 193 fields, saw 265\\nSkipping line 2530427: expected 193 fields, saw 265\\nSkipping line 2530428: expected 193 fields, saw 265\\nSkipping line 2530429: expected 193 fields, saw 265\\nSkipping line 2530430: expected 193 fields, saw 265\\nSkipping line 2530431: expected 193 fields, saw 265\\nSkipping line 2530432: expected 193 fields, saw 265\\nSkipping line 2530433: expected 193 fields, saw 265\\nSkipping line 2530434: expected 193 fields, saw 265\\nSkipping line 2530435: expected 193 fields, saw 265\\nSkipping line 2530436: expected 193 fields, saw 265\\nSkipping line 2530437: expected 193 fields, saw 265\\nSkipping line 2530438: expected 193 fields, saw 265\\nSkipping line 2530439: expected 193 fields, saw 265\\nSkipping line 2530440: expected 193 fields, saw 265\\nSkipping line 2530441: expected 193 fields, saw 265\\nSkipping line 2530442: expected 193 fields, saw 265\\nSkipping line 2530443: expected 193 fields, saw 265\\nSkipping line 2530444: expected 193 fields, saw 265\\nSkipping line 2530445: expected 193 fields, saw 265\\nSkipping line 2530447: expected 193 fields, saw 265\\nSkipping line 2530448: expected 193 fields, saw 265\\nSkipping line 2530449: expected 193 fields, saw 265\\nSkipping line 2530450: expected 193 fields, saw 265\\nSkipping line 2530453: expected 193 fields, saw 265\\nSkipping line 2530454: expected 193 fields, saw 265\\nSkipping line 2530455: expected 193 fields, saw 265\\nSkipping line 2530456: expected 193 fields, saw 265\\nSkipping line 2530457: expected 193 fields, saw 265\\nSkipping line 2530458: expected 193 fields, saw 265\\nSkipping line 2530459: expected 193 fields, saw 265\\n'\n",
      "b'Skipping line 2546956: expected 193 fields, saw 265\\nSkipping line 2546957: expected 193 fields, saw 265\\nSkipping line 2547004: expected 193 fields, saw 265\\n'\n",
      "b'Skipping line 2633922: expected 193 fields, saw 265\\nSkipping line 2633923: expected 193 fields, saw 265\\nSkipping line 2633924: expected 193 fields, saw 265\\nSkipping line 2633925: expected 193 fields, saw 265\\nSkipping line 2633926: expected 193 fields, saw 265\\nSkipping line 2633927: expected 193 fields, saw 265\\nSkipping line 2633928: expected 193 fields, saw 265\\nSkipping line 2633929: expected 193 fields, saw 265\\nSkipping line 2633930: expected 193 fields, saw 265\\nSkipping line 2633931: expected 193 fields, saw 265\\nSkipping line 2633932: expected 193 fields, saw 265\\nSkipping line 2633933: expected 193 fields, saw 265\\nSkipping line 2633934: expected 193 fields, saw 265\\nSkipping line 2633935: expected 193 fields, saw 265\\nSkipping line 2633936: expected 193 fields, saw 265\\nSkipping line 2633937: expected 193 fields, saw 265\\nSkipping line 2633938: expected 193 fields, saw 265\\nSkipping line 2633939: expected 193 fields, saw 265\\nSkipping line 2633940: expected 193 fields, saw 265\\nSkipping line 2633941: expected 193 fields, saw 265\\nSkipping line 2633942: expected 193 fields, saw 265\\nSkipping line 2633943: expected 193 fields, saw 265\\nSkipping line 2633944: expected 193 fields, saw 265\\nSkipping line 2633945: expected 193 fields, saw 265\\nSkipping line 2633946: expected 193 fields, saw 265\\nSkipping line 2633947: expected 193 fields, saw 265\\nSkipping line 2634876: expected 193 fields, saw 265\\nSkipping line 2634877: expected 193 fields, saw 265\\nSkipping line 2634878: expected 193 fields, saw 265\\nSkipping line 2634879: expected 193 fields, saw 265\\nSkipping line 2634880: expected 193 fields, saw 265\\nSkipping line 2634881: expected 193 fields, saw 265\\nSkipping line 2634882: expected 193 fields, saw 265\\nSkipping line 2634883: expected 193 fields, saw 265\\nSkipping line 2634884: expected 193 fields, saw 265\\nSkipping line 2634885: expected 193 fields, saw 265\\nSkipping line 2634886: expected 193 fields, saw 265\\nSkipping line 2634887: expected 193 fields, saw 265\\nSkipping line 2634888: expected 193 fields, saw 265\\nSkipping line 2634889: expected 193 fields, saw 265\\nSkipping line 2634890: expected 193 fields, saw 265\\nSkipping line 2634891: expected 193 fields, saw 265\\nSkipping line 2634892: expected 193 fields, saw 265\\nSkipping line 2634893: expected 193 fields, saw 265\\nSkipping line 2634894: expected 193 fields, saw 265\\nSkipping line 2634895: expected 193 fields, saw 265\\nSkipping line 2634896: expected 193 fields, saw 265\\nSkipping line 2634897: expected 193 fields, saw 265\\nSkipping line 2634898: expected 193 fields, saw 265\\n'\n",
      "b'Skipping line 2640280: expected 193 fields, saw 265\\nSkipping line 2640281: expected 193 fields, saw 265\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/parth/DATA/Linux_Backup/Projects/DeepPurpose/DeepPurpose/dataset.py:223: DtypeWarning: Columns (8,9,10,11,12,13,15,17,18,19,20,26,27,31,32,34,35,44,45,46,47,49,50,51,52,53,54,61,62,63,64,65,66,73,74,85,86,97,98,109,110,121,122,133,134,145,146,157,158,169) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, sep = '\\t', error_bad_lines=False)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 204. MiB for an array with shape (10, 2672255) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_drug, X_target, y  \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_BindingDB\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/media/parth/DATA/Linux_Backup/Projects/DeepPurpose/data/BindingDB_All.tsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mKd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mconvert_to_log\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m drug_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m target_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNN\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/media/parth/DATA/Linux_Backup/Projects/DeepPurpose/DeepPurpose/dataset.py:223\u001b[0m, in \u001b[0;36mprocess_BindingDB\u001b[0;34m(path, df, y, binary, convert_to_log, threshold, return_ids, ids_condition, harmonize_affinities)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading Dataset from path...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 223\u001b[0m \tdf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bad_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m \t\u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEither \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be provided\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torchgpu/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchgpu/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchgpu/lib/python3.9/site-packages/pandas/io/parsers/readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchgpu/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1269\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1267\u001b[0m         new_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index)\n\u001b[0;32m-> 1269\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_rows\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msqueeze \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torchgpu/lib/python3.9/site-packages/pandas/core/frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    630\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    631\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    632\u001b[0m     )\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 636\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchgpu/lib/python3.9/site-packages/pandas/core/internals/construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    494\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    495\u001b[0m         x\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m    497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m    499\u001b[0m     ]\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchgpu/lib/python3.9/site-packages/pandas/core/internals/construction.py:156\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    153\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidate\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001b[0;32m~/miniconda3/envs/torchgpu/lib/python3.9/site-packages/pandas/core/internals/managers.py:1954\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[0;34m(arrays, axes, consolidate)\u001b[0m\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_block_manager_from_column_arrays\u001b[39m(\n\u001b[1;32m   1938\u001b[0m     arrays: \u001b[38;5;28mlist\u001b[39m[ArrayLike],\n\u001b[1;32m   1939\u001b[0m     axes: \u001b[38;5;28mlist\u001b[39m[Index],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1950\u001b[0m     \u001b[38;5;66;03m# These last three are sufficient to allow us to safely pass\u001b[39;00m\n\u001b[1;32m   1951\u001b[0m     \u001b[38;5;66;03m#  verify_integrity=False below.\u001b[39;00m\n\u001b[1;32m   1953\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1954\u001b[0m         blocks \u001b[38;5;241m=\u001b[39m \u001b[43m_form_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1955\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m BlockManager(blocks, axes, verify_integrity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1956\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/torchgpu/lib/python3.9/site-packages/pandas/core/internals/managers.py:2028\u001b[0m, in \u001b[0;36m_form_blocks\u001b[0;34m(arrays, consolidate)\u001b[0m\n\u001b[1;32m   2025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(dtype\u001b[38;5;241m.\u001b[39mtype, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[1;32m   2026\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m-> 2028\u001b[0m values, placement \u001b[38;5;241m=\u001b[39m \u001b[43m_stack_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtup_block\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dtlike:\n\u001b[1;32m   2030\u001b[0m     values \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(values)\n",
      "File \u001b[0;32m~/miniconda3/envs/torchgpu/lib/python3.9/site-packages/pandas/core/internals/managers.py:2067\u001b[0m, in \u001b[0;36m_stack_arrays\u001b[0;34m(tuples, dtype)\u001b[0m\n\u001b[1;32m   2064\u001b[0m first \u001b[38;5;241m=\u001b[39m arrays[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2065\u001b[0m shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(arrays),) \u001b[38;5;241m+\u001b[39m first\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m-> 2067\u001b[0m stacked \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, arr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[1;32m   2069\u001b[0m     stacked[i] \u001b[38;5;241m=\u001b[39m arr\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 204. MiB for an array with shape (10, 2672255) and data type float64"
     ]
    }
   ],
   "source": [
    "X_drug, X_target, y  = process_BindingDB('/media/parth/DATA/Linux_Backup/Projects/DeepPurpose/data/BindingDB_All.tsv',\n",
    "                                         y = 'Kd', \n",
    "                                         binary = False, \n",
    "                                         convert_to_log = True)\n",
    "\n",
    "drug_encoding = 'Transformer'\n",
    "target_encoding = 'CNN'\n",
    "train, val, test = data_process(X_drug, X_target, y, \n",
    "                                drug_encoding, target_encoding, \n",
    "                                split_method='random',frac=[0.7,0.1,0.2])\n",
    "\n",
    "config = generate_config(drug_encoding = drug_encoding, \n",
    "                         target_encoding = target_encoding, \n",
    "                         cls_hidden_dims = [1024,1024,512], \n",
    "                         train_epoch = 100, \n",
    "                         LR = 0.001, \n",
    "                         batch_size = 128,\n",
    "                         cnn_target_filters = [32,64,96],\n",
    "                         cnn_target_kernels = [4,8,12]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 1 GPU/s!\n",
      "--- Data Preparation ---\n",
      "--- Go for Training ---\n",
      "Training at Epoch 1 iteration 0 with loss 29.634226\n",
      "Validation at Epoch 1 , MSE: 0.725780701567487 , Pearson Correlation: 0.40398057707518564 with p-value: 2.1550998167531695e-118 , Concordance Index: 0.7271688926134408\n",
      "Training at Epoch 2 iteration 0 with loss 0.603022\n",
      "Validation at Epoch 2 , MSE: 0.5726512454403675 , Pearson Correlation: 0.4879649393084924 with p-value: 1.1928492107245293e-179 , Concordance Index: 0.7650372191652788\n",
      "Training at Epoch 3 iteration 0 with loss 0.72227985\n",
      "Validation at Epoch 3 , MSE: 0.5525040086756707 , Pearson Correlation: 0.5356641450331265 with p-value: 6.8038841647049305e-223 , Concordance Index: 0.7804433797848193\n",
      "Training at Epoch 4 iteration 0 with loss 0.6410252\n",
      "Validation at Epoch 4 , MSE: 0.5703041501774694 , Pearson Correlation: 0.5541856884617206 with p-value: 1.2183007747425456e-241 , Concordance Index: 0.7877413931229313\n",
      "Training at Epoch 5 iteration 0 with loss 0.57430494\n",
      "Validation at Epoch 5 , MSE: 0.5264440393985904 , Pearson Correlation: 0.5592105171132391 with p-value: 6.214448893917246e-247 , Concordance Index: 0.7847017986774327\n",
      "Training at Epoch 6 iteration 0 with loss 0.46163726\n",
      "Validation at Epoch 6 , MSE: 0.5239591018950978 , Pearson Correlation: 0.5796159020612044 with p-value: 2.1912349762338032e-269 , Concordance Index: 0.7858452975776358\n",
      "Training at Epoch 7 iteration 0 with loss 0.4550215\n",
      "Validation at Epoch 7 , MSE: 0.603541948413404 , Pearson Correlation: 0.5977550269084122 with p-value: 1.0080912609304493e-290 , Concordance Index: 0.7991921939015735\n",
      "Training at Epoch 8 iteration 0 with loss 0.8301095\n",
      "Validation at Epoch 8 , MSE: 0.47116111386615533 , Pearson Correlation: 0.620885790442645 with p-value: 5.3774e-320 , Concordance Index: 0.8144086132017794\n",
      "Training at Epoch 9 iteration 0 with loss 0.53142715\n",
      "Validation at Epoch 9 , MSE: 0.4731906554424334 , Pearson Correlation: 0.6248141951232127 with p-value: 0.0 , Concordance Index: 0.8160135085278182\n",
      "Training at Epoch 10 iteration 0 with loss 0.38353115\n",
      "Validation at Epoch 10 , MSE: 0.46101175617190526 , Pearson Correlation: 0.6225610887191967 with p-value: 3.26e-322 , Concordance Index: 0.8121991370283699\n",
      "Training at Epoch 11 iteration 0 with loss 0.46544605\n",
      "Validation at Epoch 11 , MSE: 0.46800647421793545 , Pearson Correlation: 0.6253235457508556 with p-value: 0.0 , Concordance Index: 0.8152806766034898\n",
      "Training at Epoch 12 iteration 0 with loss 0.38787222\n",
      "Validation at Epoch 12 , MSE: 0.49059348332642927 , Pearson Correlation: 0.6305917266000525 with p-value: 0.0 , Concordance Index: 0.8156961749555514\n",
      "Training at Epoch 13 iteration 0 with loss 0.39777344\n",
      "Validation at Epoch 13 , MSE: 0.4597835762560362 , Pearson Correlation: 0.6352527111758693 with p-value: 0.0 , Concordance Index: 0.8220503130731769\n",
      "Training at Epoch 14 iteration 0 with loss 0.48289645\n",
      "Validation at Epoch 14 , MSE: 0.5861588663676301 , Pearson Correlation: 0.6272275470144072 with p-value: 0.0 , Concordance Index: 0.8210223279854391\n",
      "Training at Epoch 15 iteration 0 with loss 0.42192978\n",
      "Validation at Epoch 15 , MSE: 0.4734210217624147 , Pearson Correlation: 0.6358734159533941 with p-value: 0.0 , Concordance Index: 0.8235792240282784\n",
      "Training at Epoch 16 iteration 0 with loss 0.54570305\n",
      "Validation at Epoch 16 , MSE: 0.4483271668059146 , Pearson Correlation: 0.6346897365941144 with p-value: 0.0 , Concordance Index: 0.821825434472484\n",
      "Training at Epoch 17 iteration 0 with loss 0.39541918\n",
      "Validation at Epoch 17 , MSE: 0.4597622381052897 , Pearson Correlation: 0.640353421862969 with p-value: 0.0 , Concordance Index: 0.8175865606926261\n",
      "Training at Epoch 18 iteration 0 with loss 0.48001066\n",
      "Validation at Epoch 18 , MSE: 0.46439951930078865 , Pearson Correlation: 0.6293227898188998 with p-value: 0.0 , Concordance Index: 0.8188172439721993\n",
      "Training at Epoch 19 iteration 0 with loss 0.40129694\n",
      "Validation at Epoch 19 , MSE: 0.5442128498468414 , Pearson Correlation: 0.6418123629203649 with p-value: 0.0 , Concordance Index: 0.8187891341471127\n",
      "Training at Epoch 20 iteration 0 with loss 0.44739527\n",
      "Validation at Epoch 20 , MSE: 0.46979231787575587 , Pearson Correlation: 0.637766837211485 with p-value: 0.0 , Concordance Index: 0.8225892311260093\n",
      "Training at Epoch 21 iteration 0 with loss 0.39083403\n",
      "Validation at Epoch 21 , MSE: 0.45506637193870014 , Pearson Correlation: 0.6378444966625737 with p-value: 0.0 , Concordance Index: 0.8162544185131309\n",
      "Training at Epoch 22 iteration 0 with loss 0.3153687\n",
      "Validation at Epoch 22 , MSE: 0.43271781241925056 , Pearson Correlation: 0.6537836051217082 with p-value: 0.0 , Concordance Index: 0.8230220785107415\n",
      "Training at Epoch 23 iteration 0 with loss 0.47659552\n",
      "Validation at Epoch 23 , MSE: 0.4399627166532394 , Pearson Correlation: 0.6521851528257023 with p-value: 0.0 , Concordance Index: 0.8268768578837519\n",
      "Training at Epoch 24 iteration 0 with loss 0.3663129\n",
      "Validation at Epoch 24 , MSE: 0.4258856862760016 , Pearson Correlation: 0.6610090449894297 with p-value: 0.0 , Concordance Index: 0.8240911302960667\n",
      "Training at Epoch 25 iteration 0 with loss 0.43668756\n",
      "Validation at Epoch 25 , MSE: 0.4397800038391447 , Pearson Correlation: 0.6768974035707589 with p-value: 0.0 , Concordance Index: 0.8259165120626287\n",
      "Training at Epoch 26 iteration 0 with loss 0.34239262\n",
      "Validation at Epoch 26 , MSE: 0.4192124405888546 , Pearson Correlation: 0.6744930768817405 with p-value: 0.0 , Concordance Index: 0.8266732812598824\n",
      "Training at Epoch 27 iteration 0 with loss 0.4119158\n",
      "Validation at Epoch 27 , MSE: 0.47391782450429887 , Pearson Correlation: 0.6825673019129879 with p-value: 0.0 , Concordance Index: 0.8307496451134583\n",
      "Training at Epoch 28 iteration 0 with loss 0.39092425\n",
      "Validation at Epoch 28 , MSE: 0.4031011979798852 , Pearson Correlation: 0.6889372903149931 with p-value: 0.0 , Concordance Index: 0.8362962845838692\n",
      "Training at Epoch 29 iteration 0 with loss 0.47543424\n",
      "Validation at Epoch 29 , MSE: 0.5850640718990475 , Pearson Correlation: 0.6976275074647019 with p-value: 0.0 , Concordance Index: 0.8363037512561579\n",
      "Training at Epoch 30 iteration 0 with loss 0.46826386\n",
      "Validation at Epoch 30 , MSE: 0.4427272234655412 , Pearson Correlation: 0.7156511424054299 with p-value: 0.0 , Concordance Index: 0.845776103486321\n",
      "Training at Epoch 31 iteration 0 with loss 0.39572835\n",
      "Validation at Epoch 31 , MSE: 0.4159521700411212 , Pearson Correlation: 0.721940879091033 with p-value: 0.0 , Concordance Index: 0.848868403853857\n",
      "Training at Epoch 32 iteration 0 with loss 0.34805343\n",
      "Validation at Epoch 32 , MSE: 0.47982745623956036 , Pearson Correlation: 0.7197250691390631 with p-value: 0.0 , Concordance Index: 0.8385477058869001\n",
      "Training at Epoch 33 iteration 0 with loss 0.36586666\n",
      "Validation at Epoch 33 , MSE: 0.3795555466341415 , Pearson Correlation: 0.7290503381217495 with p-value: 0.0 , Concordance Index: 0.843455066444599\n",
      "Training at Epoch 34 iteration 0 with loss 0.26864153\n",
      "Validation at Epoch 34 , MSE: 0.412939738777511 , Pearson Correlation: 0.7283244630639841 with p-value: 0.0 , Concordance Index: 0.8455540797897385\n",
      "Training at Epoch 35 iteration 0 with loss 0.36693335\n",
      "Validation at Epoch 35 , MSE: 0.3665921494083899 , Pearson Correlation: 0.7407322026875887 with p-value: 0.0 , Concordance Index: 0.8459401506686625\n",
      "Training at Epoch 36 iteration 0 with loss 0.36277795\n",
      "Validation at Epoch 36 , MSE: 0.33976943758227934 , Pearson Correlation: 0.7405918998493471 with p-value: 0.0 , Concordance Index: 0.8449833185756752\n",
      "Training at Epoch 37 iteration 0 with loss 0.24880204\n",
      "Validation at Epoch 37 , MSE: 0.36297318059187794 , Pearson Correlation: 0.7359023640901088 with p-value: 0.0 , Concordance Index: 0.8513539272939374\n",
      "Training at Epoch 38 iteration 0 with loss 0.24418767\n",
      "Validation at Epoch 38 , MSE: 0.3426421185842073 , Pearson Correlation: 0.7467750468676866 with p-value: 0.0 , Concordance Index: 0.8473825360684193\n",
      "Training at Epoch 39 iteration 0 with loss 0.23802906\n",
      "Validation at Epoch 39 , MSE: 0.3255335815045136 , Pearson Correlation: 0.7543405491409306 with p-value: 0.0 , Concordance Index: 0.8466336727594712\n",
      "Training at Epoch 40 iteration 0 with loss 0.24160706\n",
      "Validation at Epoch 40 , MSE: 0.3680416253290795 , Pearson Correlation: 0.742739550846127 with p-value: 0.0 , Concordance Index: 0.8419628300269152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 41 iteration 0 with loss 0.33248448\n",
      "Validation at Epoch 41 , MSE: 0.33023237135946826 , Pearson Correlation: 0.7516039622218981 with p-value: 0.0 , Concordance Index: 0.8528141009423819\n",
      "Training at Epoch 42 iteration 0 with loss 0.20897366\n",
      "Validation at Epoch 42 , MSE: 0.3329422987347984 , Pearson Correlation: 0.7564631221299889 with p-value: 0.0 , Concordance Index: 0.8556202520748565\n",
      "Training at Epoch 43 iteration 0 with loss 0.24069251\n",
      "Validation at Epoch 43 , MSE: 0.4077519352355681 , Pearson Correlation: 0.7507050947749823 with p-value: 0.0 , Concordance Index: 0.8482908347915306\n",
      "Training at Epoch 44 iteration 0 with loss 0.24760874\n",
      "Validation at Epoch 44 , MSE: 0.3292789245976155 , Pearson Correlation: 0.7528507687958774 with p-value: 0.0 , Concordance Index: 0.8400832050822564\n",
      "Training at Epoch 45 iteration 0 with loss 0.20775744\n",
      "Validation at Epoch 45 , MSE: 0.31573009775373134 , Pearson Correlation: 0.7624827822071432 with p-value: 0.0 , Concordance Index: 0.8459401506686625\n",
      "Training at Epoch 46 iteration 0 with loss 0.20031127\n",
      "Validation at Epoch 46 , MSE: 0.33393605006112004 , Pearson Correlation: 0.7541164997206213 with p-value: 0.0 , Concordance Index: 0.8552783224056388\n",
      "Training at Epoch 47 iteration 0 with loss 0.14390057\n",
      "Validation at Epoch 47 , MSE: 0.3063054240751083 , Pearson Correlation: 0.774190013606482 with p-value: 0.0 , Concordance Index: 0.8582307324717672\n",
      "Training at Epoch 48 iteration 0 with loss 0.17719845\n",
      "Validation at Epoch 48 , MSE: 0.3111927910132231 , Pearson Correlation: 0.7697922966528918 with p-value: 0.0 , Concordance Index: 0.8572269042649632\n",
      "Training at Epoch 49 iteration 0 with loss 0.23493817\n",
      "Validation at Epoch 49 , MSE: 0.38880321812484453 , Pearson Correlation: 0.7686184811645655 with p-value: 0.0 , Concordance Index: 0.8534186817897526\n",
      "Training at Epoch 50 iteration 0 with loss 0.26537552\n",
      "Validation at Epoch 50 , MSE: 0.3117944822270837 , Pearson Correlation: 0.7697944636844831 with p-value: 0.0 , Concordance Index: 0.8575815711986732\n",
      "Training at Epoch 51 iteration 0 with loss 0.2059507\n",
      "Validation at Epoch 51 , MSE: 0.35994106974982654 , Pearson Correlation: 0.7724793895602798 with p-value: 0.0 , Concordance Index: 0.8604639263100935\n",
      "Training at Epoch 52 iteration 0 with loss 0.29771197\n",
      "Validation at Epoch 52 , MSE: 0.30243781151683097 , Pearson Correlation: 0.774970433073671 with p-value: 0.0 , Concordance Index: 0.8596504982466496\n",
      "Training at Epoch 53 iteration 0 with loss 0.13921075\n",
      "Validation at Epoch 53 , MSE: 0.3166499760629699 , Pearson Correlation: 0.7697556007106207 with p-value: 0.0 , Concordance Index: 0.8538225409173641\n",
      "Training at Epoch 54 iteration 0 with loss 0.2019538\n",
      "Validation at Epoch 54 , MSE: 0.31847235681748987 , Pearson Correlation: 0.7654841044450228 with p-value: 0.0 , Concordance Index: 0.8566304489139066\n",
      "Training at Epoch 55 iteration 0 with loss 0.17592895\n",
      "Validation at Epoch 55 , MSE: 0.32140615317544025 , Pearson Correlation: 0.7796043192450737 with p-value: 0.0 , Concordance Index: 0.856714558781158\n",
      "Training at Epoch 56 iteration 0 with loss 0.1588264\n",
      "Validation at Epoch 56 , MSE: 0.2933640105662261 , Pearson Correlation: 0.7816763328240687 with p-value: 0.0 , Concordance Index: 0.8673932178019522\n",
      "Training at Epoch 57 iteration 0 with loss 0.18687354\n",
      "Validation at Epoch 57 , MSE: 0.3086026125372463 , Pearson Correlation: 0.7743889549515655 with p-value: 0.0 , Concordance Index: 0.8571318139972874\n",
      "Training at Epoch 58 iteration 0 with loss 0.19043419\n",
      "Validation at Epoch 58 , MSE: 0.3167679398534858 , Pearson Correlation: 0.7785929443516493 with p-value: 0.0 , Concordance Index: 0.8625517396468001\n",
      "Training at Epoch 59 iteration 0 with loss 0.18739408\n",
      "Validation at Epoch 59 , MSE: 0.3766276346080875 , Pearson Correlation: 0.7619611139553615 with p-value: 0.0 , Concordance Index: 0.8616711115327584\n",
      "Training at Epoch 60 iteration 0 with loss 0.22643688\n",
      "Validation at Epoch 60 , MSE: 0.2929906934487656 , Pearson Correlation: 0.7820891683717808 with p-value: 0.0 , Concordance Index: 0.8673539079684327\n",
      "Training at Epoch 61 iteration 0 with loss 0.16580065\n",
      "Validation at Epoch 61 , MSE: 0.4560569024952441 , Pearson Correlation: 0.7672895648856787 with p-value: 0.0 , Concordance Index: 0.8589492898755438\n",
      "Training at Epoch 62 iteration 0 with loss 0.37498075\n",
      "Validation at Epoch 62 , MSE: 0.3485699448628535 , Pearson Correlation: 0.7723602560927123 with p-value: 0.0 , Concordance Index: 0.851666209882009\n",
      "Training at Epoch 63 iteration 0 with loss 0.14209083\n",
      "Validation at Epoch 63 , MSE: 0.31027944286285397 , Pearson Correlation: 0.7856141845805611 with p-value: 0.0 , Concordance Index: 0.8625548141589189\n",
      "Training at Epoch 64 iteration 0 with loss 0.13787936\n",
      "Validation at Epoch 64 , MSE: 0.2978461283957146 , Pearson Correlation: 0.7863738487357339 with p-value: 0.0 , Concordance Index: 0.8635557874616125\n",
      "Training at Epoch 65 iteration 0 with loss 0.16634636\n",
      "Validation at Epoch 65 , MSE: 0.28281064973311476 , Pearson Correlation: 0.7914869465988887 with p-value: 0.0 , Concordance Index: 0.8665721034582112\n",
      "Training at Epoch 66 iteration 0 with loss 0.16248938\n",
      "Validation at Epoch 66 , MSE: 0.3356534599631882 , Pearson Correlation: 0.7775138080960353 with p-value: 0.0 , Concordance Index: 0.8527831362131849\n",
      "Training at Epoch 67 iteration 0 with loss 0.17652586\n",
      "Validation at Epoch 67 , MSE: 0.3067052467119645 , Pearson Correlation: 0.7867424585931191 with p-value: 0.0 , Concordance Index: 0.8649564473397564\n",
      "Training at Epoch 68 iteration 0 with loss 0.14171846\n",
      "Validation at Epoch 68 , MSE: 0.31755287723766024 , Pearson Correlation: 0.782436550684411 with p-value: 0.0 , Concordance Index: 0.866221828684671\n",
      "Training at Epoch 69 iteration 0 with loss 0.14237842\n",
      "Validation at Epoch 69 , MSE: 0.34041388161182545 , Pearson Correlation: 0.7899690388915522 with p-value: 0.0 , Concordance Index: 0.8655480713146263\n",
      "Training at Epoch 70 iteration 0 with loss 0.2093993\n",
      "Validation at Epoch 70 , MSE: 0.28555122295025404 , Pearson Correlation: 0.7976450577205181 with p-value: 0.0 , Concordance Index: 0.8718806878474199\n",
      "Training at Epoch 71 iteration 0 with loss 0.13721466\n",
      "Validation at Epoch 71 , MSE: 0.2861693923533604 , Pearson Correlation: 0.7880974203176389 with p-value: 0.0 , Concordance Index: 0.8708390870631557\n",
      "Training at Epoch 72 iteration 0 with loss 0.080976\n",
      "Validation at Epoch 72 , MSE: 0.2939428775194183 , Pearson Correlation: 0.7843787271576592 with p-value: 0.0 , Concordance Index: 0.8657731695233276\n",
      "Training at Epoch 73 iteration 0 with loss 0.15536982\n",
      "Validation at Epoch 73 , MSE: 0.29759863823311794 , Pearson Correlation: 0.7787653372195861 with p-value: 0.0 , Concordance Index: 0.8576588732176614\n",
      "Training at Epoch 74 iteration 0 with loss 0.10382487\n",
      "Validation at Epoch 74 , MSE: 0.31064877717782885 , Pearson Correlation: 0.7745366882267296 with p-value: 0.0 , Concordance Index: 0.8602845065671579\n",
      "Training at Epoch 75 iteration 0 with loss 0.10860255\n",
      "Validation at Epoch 75 , MSE: 0.29593183945611423 , Pearson Correlation: 0.7865838770327085 with p-value: 0.0 , Concordance Index: 0.8653526201870709\n",
      "Training at Epoch 76 iteration 0 with loss 0.09611059\n",
      "Validation at Epoch 76 , MSE: 0.29878144466376433 , Pearson Correlation: 0.7851372671838887 with p-value: 0.0 , Concordance Index: 0.8606042558275181\n",
      "Training at Epoch 77 iteration 0 with loss 0.13595721\n",
      "Validation at Epoch 77 , MSE: 0.3402778819122518 , Pearson Correlation: 0.7823734120003081 with p-value: 0.0 , Concordance Index: 0.8628078025846985\n",
      "Training at Epoch 78 iteration 0 with loss 0.21282764\n",
      "Validation at Epoch 78 , MSE: 0.28612222489712985 , Pearson Correlation: 0.7928125245940223 with p-value: 0.0 , Concordance Index: 0.8663294366088307\n",
      "Training at Epoch 79 iteration 0 with loss 0.09100406\n",
      "Validation at Epoch 79 , MSE: 0.29902305999838713 , Pearson Correlation: 0.7925053616587834 with p-value: 0.0 , Concordance Index: 0.874747231182229\n",
      "Training at Epoch 80 iteration 0 with loss 0.13347423\n",
      "Validation at Epoch 80 , MSE: 0.31253688722776996 , Pearson Correlation: 0.784692023213976 with p-value: 0.0 , Concordance Index: 0.8569089118686709\n",
      "Training at Epoch 81 iteration 0 with loss 0.12549114\n",
      "Validation at Epoch 81 , MSE: 0.3010081206304513 , Pearson Correlation: 0.7825805965200501 with p-value: 0.0 , Concordance Index: 0.8637305954363699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at Epoch 82 iteration 0 with loss 0.1079582\n",
      "Validation at Epoch 82 , MSE: 0.29168211834795316 , Pearson Correlation: 0.7942657867509696 with p-value: 0.0 , Concordance Index: 0.869972294253649\n",
      "Training at Epoch 83 iteration 0 with loss 0.07591695\n",
      "Validation at Epoch 83 , MSE: 0.29756025245344164 , Pearson Correlation: 0.7943579943692158 with p-value: 0.0 , Concordance Index: 0.8691799485590201\n",
      "Training at Epoch 84 iteration 0 with loss 0.07741684\n",
      "Validation at Epoch 84 , MSE: 0.29357583706887136 , Pearson Correlation: 0.8010128081235468 with p-value: 0.0 , Concordance Index: 0.8742335680503728\n",
      "Training at Epoch 85 iteration 0 with loss 0.090614535\n",
      "Validation at Epoch 85 , MSE: 0.2859333284412329 , Pearson Correlation: 0.7988325976461027 with p-value: 0.0 , Concordance Index: 0.8749890195995755\n",
      "Training at Epoch 86 iteration 0 with loss 0.11960477\n",
      "Validation at Epoch 86 , MSE: 0.29872324837589437 , Pearson Correlation: 0.7852945263006591 with p-value: 0.0 , Concordance Index: 0.8693868193030169\n",
      "Training at Epoch 87 iteration 0 with loss 0.09527132\n",
      "Validation at Epoch 87 , MSE: 0.29218811961414093 , Pearson Correlation: 0.7907322467554618 with p-value: 0.0 , Concordance Index: 0.8661140011525028\n",
      "Training at Epoch 88 iteration 0 with loss 0.10724583\n",
      "Validation at Epoch 88 , MSE: 0.28132808310724317 , Pearson Correlation: 0.7965797527606073 with p-value: 0.0 , Concordance Index: 0.8731003907265688\n",
      "Training at Epoch 89 iteration 0 with loss 0.11214432\n",
      "Validation at Epoch 89 , MSE: 0.2812944953479944 , Pearson Correlation: 0.7984507127024164 with p-value: 0.0 , Concordance Index: 0.8746686115151898\n",
      "Training at Epoch 90 iteration 0 with loss 0.110264264\n",
      "Validation at Epoch 90 , MSE: 0.2943626711714906 , Pearson Correlation: 0.7958484242200626 with p-value: 0.0 , Concordance Index: 0.8698635882894469\n",
      "Training at Epoch 91 iteration 0 with loss 0.11829962\n",
      "Validation at Epoch 91 , MSE: 0.28645835748872467 , Pearson Correlation: 0.7907913600411716 with p-value: 0.0 , Concordance Index: 0.8659257970892276\n",
      "Training at Epoch 92 iteration 0 with loss 0.12059486\n",
      "Validation at Epoch 92 , MSE: 0.2956916409092075 , Pearson Correlation: 0.7918484698618803 with p-value: 0.0 , Concordance Index: 0.8699617530692415\n",
      "Training at Epoch 93 iteration 0 with loss 0.0967102\n",
      "Validation at Epoch 93 , MSE: 0.3069302997519912 , Pearson Correlation: 0.7874464559359134 with p-value: 0.0 , Concordance Index: 0.8596871727840674\n",
      "Training at Epoch 94 iteration 0 with loss 0.13414422\n",
      "Validation at Epoch 94 , MSE: 0.28288662992067215 , Pearson Correlation: 0.7947025411715298 with p-value: 0.0 , Concordance Index: 0.8659552245623652\n",
      "Training at Epoch 95 iteration 0 with loss 0.10370281\n",
      "Validation at Epoch 95 , MSE: 0.28695570933638226 , Pearson Correlation: 0.7927755234564616 with p-value: 0.0 , Concordance Index: 0.8643083841067049\n",
      "Training at Epoch 96 iteration 0 with loss 0.13430275\n",
      "Validation at Epoch 96 , MSE: 0.3258646538582383 , Pearson Correlation: 0.7833035914012931 with p-value: 0.0 , Concordance Index: 0.8716263817735894\n",
      "Training at Epoch 97 iteration 0 with loss 0.14196697\n",
      "Validation at Epoch 97 , MSE: 0.2896430110049083 , Pearson Correlation: 0.7894914155305123 with p-value: 0.0 , Concordance Index: 0.864822925670595\n",
      "Training at Epoch 98 iteration 0 with loss 0.0765197\n",
      "Validation at Epoch 98 , MSE: 0.2824313373215666 , Pearson Correlation: 0.7947989487940375 with p-value: 0.0 , Concordance Index: 0.8668753821179348\n",
      "Training at Epoch 99 iteration 0 with loss 0.0673421\n",
      "Validation at Epoch 99 , MSE: 0.3075603177427848 , Pearson Correlation: 0.7977542563703856 with p-value: 0.0 , Concordance Index: 0.8696652822577812\n",
      "Training at Epoch 100 iteration 0 with loss 0.11071129\n",
      "Validation at Epoch 100 , MSE: 0.2819463625926039 , Pearson Correlation: 0.7962812176691273 with p-value: 0.0 , Concordance Index: 0.8602355339812648\n",
      "--- Go for Testing ---\n",
      "Testing MSE: 0.2565965372661101 , Pearson Correlation: 0.8231646722825967 with p-value: 0.0 , Concordance Index: 0.8681803072880361\n",
      "--- Training Finished ---\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAELCAYAAAA7h+qnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAb80lEQVR4nO3de5hcVZ3u8e+bhFuDI7m0DgJJA6IjOA6XDAfUwyjwKDKjoKNn5DQaBG0N8ghHnnNAMx7RITI6ircDOFFAnJS3ARwZQB1E1ANHGTsYIhowAZIQiNAEuUiPMCS/88dabVV3Vad3d+rW2e/neeqp2mvv2nvVTqXe3mvtvbYiAjMzK7cZna6AmZl1nsPAzMwcBmZm5jAwMzMcBmZmBszqdAWmat68edHX19fpapiZTSsrVqx4JCJ6x5ZP2zDo6+tjcHCw09UwM5tWJK1vVO5mIjMzcxiYmZnDwMzMcBiYmRkOAzMzo81hIGlXSf8u6Q5Jv5T0kVy+n6TbJK2R9A1JO7di+5UK9PXBjBnpuVJpxVbMzKafdh8ZPA0cExF/BhwCHC/pSODjwKcj4kDgt8Dpzd5wpQIDA7B+PUSk54EBB4KZGbQ5DCL5XZ7cKT8COAa4KpdfCZzU7G0vWQLDw6PLhodTuZlZ2bW9z0DSTEkrgYeBG4F7gMci4tm8yEZg73HeOyBpUNLg0NDQpLa7vuFlFuOXm5mVSdvDICK2RMQhwD7AEcBLGi02znuXRcTCiFjY21t3NfU2zZw5uXIzszLp2NlEEfEY8EPgSGBPSSNDY+wDPNjs7W3ZMrlyM7MyaffZRL2S9syvdwOOA1YDNwNvzostAr7d7G0vWDC5cjOzMmn3kcFewM2SVgE/A26MiOuAc4H3S1oLzAUua/aGly6Fnp7RZT09qdzMrOzaOmppRKwCDm1Qfi+p/6Bl+vvT8xlnwBNPpCOCpUur5WZmZTZth7Ceiv5+WLkSLr4Y1q3rdG3MzLpH6YajkDpdAzOz7lO6MDAzs3oOAzMzK2cYRMNL2szMyqt0YeA+AzOzeqULAzMzq1fKMHAzkZnZaKULAzcTmZnVK10YmJlZvVKGgZuJzMxGK10YuJnIzKxe6cLAzMzqOQzMzKycYeA+AzOz0UoXBu4zMDOrV7owMDOzeqUMAzcTmZmNVrowcDORmVm90oWBmZnVK2UYuJnIzGy00oWBm4nMzOqVLgzMzKyew8DMzNobBpL2lXSzpNWSfinprFx+vqQHJK3MjxNasf1KBT7/ediyBfr60rSZmcGsNm/vWeCciLhd0nOAFZJuzPM+HRGfbNWGKxUYGIDh4TS9fn2aBujvb9VWzcymh7YeGUTEpoi4Pb9+ElgN7N2ObS9ZUg2CEcPDqdzMrOw61mcgqQ84FLgtF50paZWkyyXNHuc9A5IGJQ0ODQ1NansbNkyu3MysTDoSBpL2AK4Gzo6IJ4BLgQOAQ4BNwKcavS8ilkXEwohY2NvbO6ltzp8/uXIzszJpexhI2okUBJWIuAYgIh6KiC0RsRX4InBEs7e7dCn09Iwu6+lJ5WZmZdfus4kEXAasjoiLasr3qlnsjcCdzd52fz8sWwbPfW6aXrAgTbvz2Mys/WcTvQJ4G/ALSStz2QeBkyUdAgSwDnh3Kzbe3w+//jV89KOwbl0rtmBmNj21NQwi4hag0YAQN7SzHmZmNpqvQDYzs/KFgQeqMzOrV7owMDOzeg4DMzMrbxj4BjdmZlWlCwP3GZiZ1StdGJiZWb3ShoGbiczMqkoXBm4mMjOrV7owMDOzeqUNAzcTmZlVlS4M3ExkZlavdGFgZmb1HAZmZlbeMHCfgZlZVenCwH0GZmb1ShcGZmZWr7Rh4GYiM7Oq0oWBm4nMzOqVLgzMzKxeacPAzURmZlWlCwM3E5mZ1StdGJiZWb3CYSDpUEnXSHpE0rOSDsvlH5N0fOuqaGZmrVYoDCS9EvgJ8CfAV8e8byvwnoLr2VfSzZJWS/qlpLNy+RxJN0pak59nT+5jTJ77DMzMqooeGfw98D3gYOD9Y+bdDhxWcD3PAudExEuAI4H3SjoIOA+4KSIOBG7K0y3hPgMzs3pFw+Aw4NKICGDs39SPAL1FVhIRmyLi9vz6SWA1sDdwInBlXuxK4KSC9TIzsyYoGga/B3rGmbcX8PhkNyypDzgUuA14fkRsghQYwPPGec+ApEFJg0NDQ5Pd5ChuJjIzqyoaBrcAZ0uaWVM28nN6OvCDyWxU0h7A1cDZEfFE0fdFxLKIWBgRC3t7Cx2MjFKpwCc+kV6/6EVp2szMYFbB5T4E3ArcAVxFCoJFki4CDgf+vOgGJe1ECoJKRFyTix+StFdEbJK0F/Bw0fUVVanAwAAMD6fp++9P0wD9/c3empnZ9FLoyCAi7gCOBh4ClgACzsyz/yIi7i6yHkkCLgNWR8RFNbOuBRbl14uAbxdZ32QsWVINghHDw6nczKzsih4ZkDt+j5W0KzAHeCwihid421ivAN4G/ELSylz2QdLZSt+UdDqwAXjLJNc7oQ0bJlduZlYmhcNgRET8HnhwKhuLiFtIRxWNHDuVdRY1fz6sX9+43Mys7AqFgaT/PcEiERF/14T6tMzSpaP7DAB6elK5mVnZFT0yOH8b80bOKurqMBjpJD7zTHjsMdh3X7jwQncem5lBwTCIiLqO5jxkxOuBc5gmF4n198MDD8C558Jdd6UjAzMzm0KfwYiI+C3wFUlzgYuBE5pWKzMza6tmDGE9ctrptOIrkM3MqpoRBn8FbN/YEG3kgerMzOoVPZvo8gbFOwMvBf4U+HAzK2VmZu1VtM/gGOpHK/09sB74DNURR6cNNxOZmVUVPZuor8X1aBs3E5mZ1fM9kM3MbPwjA0mTOkMoIn68/dUxM7NO2FYz0Q+p7ydoRHm5mRMt2E3cZ2BmVrWtMHh122rRRu4zMDOrN24YRMSP2lkRMzPrnNJ2ILuZyMysqvDYRJJeSrrf8YuBXcfMjoho6f0ImsXNRGZm9YpegfxfgB8B64ADgVXAbGA+sBFY26L6mZlZGxRtJvoYcA1wMOnsodPzhWjHkc4iuqAltWshNxOZmVUVDYOXAcupnmo6EyAifkAKggubX7XWcDORmVm9omGwE/BURGwFHgX2qpl3N2nAOjMzm6aKhsE9wN759SrgNEkzJM0A3gH8phWVayU3E5mZVRU9m+hfgVcBXyX1H1wPPAFsAfYA3teKyrWCm4nMzOoVHbX0/JrX35d0JPDXQA/w3Yj4t9ZUz8zM2mFK90COiJ8DP29yXczMrEMK9RlIukbSSZJ22p6NSbpc0sOS7qwpO1/SA5JW5scJ27ONotxnYGZWVbQD+U9I1xlsknRxbiaaii8Dxzco/3REHJIfN0xx3YW4z8DMrF6hMIiIg4A/J11r8CbgVklrJH1I0v5FN5bvefDolGpqZmYtU3iguohYERFnA/sArwd+BpwLrJH0f7ezHmdKWpWbkWaPt5CkAUmDkgaHhoa2a4NuJjIzq5r0qKURsSUiboiI/046SngQePl21OFS4ADgEGAT8KltbHtZRCyMiIW9vb1T2pibiczM6k06DCQdIOnDkn4NfIc0VtG4P+ATiYiHcsBsBb4IHDHVdZmZ2dQUHbV0NvA3wNuAI4Fh4FvAe4HvR0y90UXSXhGxKU++EbhzW8s3i5uJzMyqil5n8BvS4HQ/AE4Fro6I4cluTNLXSFcyz5O0Efgw8CpJh5AGwVsHvHuy651cHVq5djOz6aloGPwtsLzmL/gpiYiTGxRftj3rNDOz7Vd0OIp/aHVFzMysc3wPZDMzK18YuM/AzKxe6cLAzMzqlTYM3ExkZlZVujBwM5GZWb2iQ1ifKOkdNdMLJP1E0pOSrpK0R+uqaGZmrVb0yOBvgdrBgC4iDVi3DDgaOL+51Wo9NxOZmVUVDYMDgFUAknYDTgDeHxHnAB8kDSMxLbiZyMysXtEw2BX4j/z65aSL1Ubue3w38IIm18vMzNqoaBisA16ZX58IrIiIx/P084DHG73JzMymh6JjE/0j8ElJbyTdd2BxzbyjgF81u2Kt5j4DM7OqomMTfVbSI6Thqz8XEV+pmf0c4IpWVK4V3GdgZlav6JEBEVEBKg3KWzrktJmZtV7R6wxeJOmImundJF0o6V8lndm66rWOm4nMzKqKdiD/H+DNNdNLgXNIZxF9WtJ7m12xVnEzkZlZvaJh8DLgVgBJM4C3A+dGxOHABcBAa6pnZmbtUDQM9gQ259eHArOBq/L0D4H9m1ut1nMzkZlZVdEweAh4YX79GuCeiLg/T+8BPNvsirWKm4nMzOoVPZvoWuBCSS8FTiVddzDiT4F7m1wvMzNro6JhcB5pSIrXkoLhYzXz3kB1aAozM5uGil509hTwrnHmvbypNWoT9xmYmVUVvugMQNIc0vATc0gdyj+NiEdbUbFWcZ+BmVm9wmEg6QLStQW71BQ/LemTEfGhptfMzMzapugVyGeT7luwHHg18JL8vBz4oKT3FVzP5ZIelnRnTdkcSTdKWpOfZ0/6U0yBm4nMzKqKnlr6HuCzEfGuiPhRRNydn98FfA44o+B6vgwcP6bsPOCmiDgQuClPt4ybiczM6hUNgz7g+nHmXZ/nTygifgyM7WM4Ebgyv74SOKlgnczMrEmKhsFm4KXjzDuY6tXJU/H8iNgEkJ+fN96CkgYkDUoaHBoa2o5NupnIzKxW0TD4FvB3kt4maScASbMknQx8FLi6VRWsFRHLImJhRCzs7e2d0jrcTGRmVq9oGHwAWElqxhmW9BDpnsgV4A5S5/JUPSRpL4D8/PB2rMvMzKag6EVnT0o6GvhL4L+SrjN4FPgR8J2I7Wp0uRZYBPx9fv72dqzLzMymYDJ3OgvguvyYEklfA14FzJO0EfgwKQS+Kel0YAPwlqmufzLcZ2BmVjWpK5C3V0ScPM6sY9ux/UoFzj03vT7qKPj4x6G/vx1bNjPrbuOGgaStQNG/nyMi2hosk1WpwMAADA+n6QceSNPgQDAz29YP+EcpHgZdb8mSahCMGB5O5Q4DMyu7ccMgIs5vYz1absOGyZWbmZVJ0VNLp7358ydXbmZWJqUJg6VLoadndFlPTyo3Myu70oRBfz8sWgQz8ieeOTNNu7/AzKxEYVCpwJVXwtataXrLljRdqXS2XmZm3aA0YbCts4nMzMquNGHgs4nMzMZXmjDw2URmZuMrTRj4bCIzs/GVJgz6+2HZMpg7N02/4AVp2mcTmZm1eaC6Tuvvh2efhVNPhVtugf3263SNzMy6Q2mODMzMbHylDQPfz8DMrKp0YeB7IJuZ1StdGJiZWb3ShoGbiczMqkoXBm4mMjOrV7owMDOzeg4DMzMrbxi4z8DMrKp0YeA+AzOzeqULAzMzq9c1YxNJWgc8CWwBno2Iha3cnpuJzMyquiYMsldHxCOt3ICbiczM6rmZyMzMuioMAvg3SSskDTRaQNKApEFJg0NDQ1PayK23pucXvxj6+qBSmWJtzcx2IN0UBq+IiMOA1wHvlXT02AUiYllELIyIhb29vZPeQKUCl102si5Yvx4GBhwIZmZdEwYR8WB+fhj4FnBEs7exZAk888zosuHhVG5mVmZdEQaSdpf0nJHXwGuAO5u9nQ0bJlduZlYW3XI20fOBbymd6jML+GpEfLfZG5kzBzZvblxuZlZmXREGEXEv8GedroeZWVl1RTNRuzQ6KgB49NH21sPMrNuUJgwqlfEvOJs/v711MTPrNqUJgyVLGg9BIcHSpe2vj5lZNylNGIx3xlAE9Pe3ty5mZt2mNGEwXlPQggXtrYeZWTcqTRgsXQo9PaPLenrcRGRmBl1yamk7jDQFLVoEW7ak17vt1rn6mJl1k9IcGYyo7UTevNljE5mZQcnCYMkS2Lp1dJnHJjIzK1kYeGwiM7PGShUGu+8+uXIzs7IoVRg89dTkys3MyqJUYdDoCuRtlZuZlUVpwsBnDJmZja80YeAzhszMxleaMFi/vtM1MDPrXqUJgxkTfNK+PjclmVl5lSYMxl5sNtb69a29GrlSSYEzY4aDx8y6T2nCoIjhYTjttOb/aFcqab3r16czl9avT9OdCASHkpk1UpowmDu32HLPPDP6R/uUU9INcMZ7HHxw+lHd1jKnnJLWO3Y7Z52VfoznzasuO29eKjvjDJg1q35dM2em57E/5LU/8vPmpcfID/5xx1XXdcopoz/fREdDjcJjokBx4HSW979NhWKanmS/cOHCGBwcLLx8pZJ+CK015s6Fz34WrrgCbrpp9DwJ3vMeuOSSztStTCqVFPDDw9Wynh5Ytsw3cbJE0oqIWDi2vDRHBv6P0FqbN6ewHRsEkI5CLr1020dPk33suuv4R0sjfxlPtI4ZMxofZXWzif7qX7JkdBBAmj7rrHbV0KatiJiWj8MPPzwmK/0s+eGHHyOPGTMiFi+OWL48YsGCCCk9H3tsxMyZaZmZM9MyIxYvHn/eeEbWP/IeSNPLl0/6v3FdXaeyjm7aTrsBgxH1v6mlaSaC9FegmdmOYio/313fTCTpeEl3S1or6bxO18fMrNs18w/crggDSTOBi4HXAQcBJ0s6qNnbWby42Ws0M+usZvV3dUUYAEcAayPi3oh4Bvg6cGKzN3LJJQ4EM9uxNGvctW4Jg72B+2umN+ayUSQNSBqUNDg0NDSlDV1ySWpnW77cN7Uxs+mvWXdq7JYwaNTyVdc1EhHLImJhRCzs7e3drg3298Pvflc9r2L5cliwYLtWaWbWdvPnN2c93RIGG4F9a6b3AR5sZwX6+2Hdus6e6DcSSFJ6Xr68c+tZvnz0Vdtz5zZeT+225s710ZZZuy1d2qQVNTrftN0PYBZwL7AfsDNwB3Dwtt4zlesMzHYkzToPvtF6xpYtXhwxd+7o6xMgle2+++T/ZNlll/qynXcePS1NvJ6dd67WpYyPqfyb0+3XGUg6AfgMMBO4PCK2mXdTuc7AzKzsxrvOYFYnKtNIRNwA3NDpepiZlVG39BmYmVkHOQzMzMxhYGZmDgMzM2Ma39xG0hCwfopvnwc80sTq7Ii8j4rxfpqY91Ex7dpPCyKi7qrdaRsG20PSYKNTq6zK+6gY76eJeR8V0+n95GYiMzNzGJiZWXnDYFmnKzANeB8V4/00Me+jYjq6n0rZZ2BmZqOV9cjAzMxqOAzMzKx8YSDpeEl3S1or6bxO16edJO0r6WZJqyX9UtJZuXyOpBslrcnPs3O5JH0u76tVkg6rWdeivPwaSYs69ZlaRdJMST+XdF2e3k/SbfnzfkPSzrl8lzy9Ns/vq1nHB3L53ZJe25lP0hqS9pR0laS78vfpKH+P6kn6H/n/2p2SviZp1679LjUa13pHfZCGx74H2J/qfRMO6nS92vj59wIOy6+fA/waOAj4BHBeLj8P+Hh+fQLwHdKd6I4Ebsvlc0j3n5gDzM6vZ3f68zV5X70f+CpwXZ7+JvDW/PoLwOL8+gzgC/n1W4Fv5NcH5e/XLqT7dNwDzOz052ri/rkSeGd+vTOwp79Hdftob+A+YLea79Cp3fpdKtuRwRHA2oi4NyKeAb4OnNjhOrVNRGyKiNvz6yeB1aQv7Imk/9zk55Py6xOBr0TyU2BPSXsBrwVujIhHI+K3wI3A8W38KC0laR/gL4Ev5WkBxwBX5UXG7qORfXcVcGxe/kTg6xHxdETcB6wlff+mPUl/BBwNXAYQEc9ExGP4e9TILGA3SbOAHmATXfpdKlsY7A3cXzO9MZeVTj4EPRS4DXh+RGyCFBjA8/Ji4+2vHX0/fgb4X8DWPD0XeCwins3TtZ/3D/siz388L78j76P9gSHgityU9iVJu+Pv0SgR8QDwSWADKQQeB1bQpd+lsoWBGpSV7txaSXsAVwNnR8QT21q0QVlso3zak/RXwMMRsaK2uMGiMcG8HXYfkf7aPQy4NCIOBZ4iNQuNp4z7iNxnciKpaecFwO7A6xos2hXfpbKFwUZg35rpfYAHO1SXjpC0EykIKhFxTS5+KB+2k58fzuXj7a8deT++AniDpHWkZsRjSEcKe+ZDfRj9ef+wL/L85wKPsmPvo43Axoi4LU9fRQoHf49GOw64LyKGIuI/gWuAl9Ol36WyhcHPgANzb/7OpE6aaztcp7bJ7Y+XAasj4qKaWdcCI2dyLAK+XVP+9nw2yJHA4/nw/3vAayTNzn/9vCaXTXsR8YGI2Cci+kjfjx9ERD9wM/DmvNjYfTSy796cl49c/tZ8hsh+wIHAv7fpY7RURPwGuF/Si3PRscCv8PdorA3AkZJ68v+9kf3Und+lTve4t/tBOrPh16Qe+SWdrk+bP/srSYeXq4CV+XECqV3yJmBNfp6Tlxdwcd5XvwAW1qzrNFJH1lrgHZ3+bC3aX6+iejbR/vk/4Frgn4FdcvmueXptnr9/zfuX5H13N/C6Tn+eJu+bQ4DB/F36F9LZQP4e1e+njwB3AXcC/0Q6I6grv0sejsLMzErXTGRmZg04DMzMzGFgZmYOAzMzw2FgZmY4DGwHJOl8SZFf75mnD5vofS2szyG5DnMazAtJ53egWmajOAxsR/Ql4Kj8ek/gw6QrZDvlkFyHujAg1fNL7a2OWb1ZEy9iNr1ExEbSJfwtka8m3SnSyLfbJdIonmYd5yMD2+GMNBPlkVnvy8VfzGUh6dSaZd8k6aeShiU9JumfJc0fs751kpZLOk3SXcAzpCGukfQRSbdLelzSI5J+kIdcGHnvqcAVeXJNTR368vy6ZiKlGzD9RNJ/5PX+S83QDyPL/FDSLZKOy9sfzjdQOQmzKXAY2I5sE/Cm/PpCUpPMUcD1AJLeQxq071eksWDeDbwU+JGk54xZ16tJN7z5CGnM/VW5fG/g06Qx6U8lDc72Y0kvy/OvBy7Ir99SU4dNjSos6fj8nt8BfwMsznW6RdLYYYsPAD4LXJQ/5ybgKkkv3OZeMWvAzUS2w4qIpyX9PE/eW9skk4fx/jhwRUScVlN+G2nsqtNJo5WOmA0cHmmQttptvLPmvTOB7wK/zO8/KyKGJN2TF1kZEWsnqPYFpDt+vS7ymPeSfpLrdA4pkEbMA46OiDV5udtJgfDfgI9NsB2zUXxkYGV1FPBHQEXSrJEHqa/hLtKdvGr9dGwQAORmmpslbQaeBf4TeBHw4rHLTiTfIOYw0u0OR25+QqS7W90K/MWYt6wZCYK83MOkI5P5mE2SjwysrEbuwvX9ceb/dsx0XbNOPl31BtKwy6fnZbaQzg7adQp1mk0a4bNRE9JvgAVjyh5tsNzTU9y2lZzDwMpqc34+ldSsM9aTY6YbDe/716SjgTdFunkJ8Ic7XD02hTr9Nm/njxvM+2OqdTZrOoeB7eiezs+7jSn/f6Qf/BdGxJVMTQ/pSOAPQSHpGFIzzX01y41Xh1Ei4ilJK4C3SDo/IrbkdS4g3SHr81Osp9mEHAa2o3uI9Bf1WyWtIt2v976I2CzpfwIXS+oFvkO6AfnepLb5H0bEVydY93eBs4EvS7qC1FfwIeCBMcv9Kj+/V9KVpH6FVeNcp/Ah0tlE10m6BNiDdAbT48CnJvG5zSbFHci2Q4uIrcA7Se3x3yfd+vT1ed4/Am8gdfb+EykQPkL6I2llgXV/D3gf6b7J15Hu2vV20p2qape7Azg/b/eWXIcXjLPO75KuYdgT+CbwBWA18MqI2JHuD2xdxnc6MzMzHxmYmZnDwMzMcBiYmRkOAzMzw2FgZmY4DMzMDIeBmZnhMDAzM+D/A1JmzTefoXRIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = models.model_initialize(**config)\n",
    "model.train(train, val, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model('./model_DeepDTA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
